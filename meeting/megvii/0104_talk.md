# Jan 4th, 2017

* Anbang Yao, Intel

## Lossless Efficient Deep Neural Network (Network Compression)

### Related work
* Model compression
* Speed up feed-forward
* Learning efficient DNNs for memory & computation
    * Pruning with re-training [NIPS 2015, ICLR 2016]

### Works
* Dynamic Network Surgery (NIPS 2016)
* Incremental Network Quantization (ICLR 2017, submit)

###
* Pruning: remove redundant parameter
    * Metric: absolute value, the bigger, the more important
* Re-training: train again
* Loop these two operations
* Disadvantage
    * Too much time
    * Metric
    * Cannot recover pruned parameters

###
* Dynamic: 