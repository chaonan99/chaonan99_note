## Language Modeling with Gated Convolutional Networks
* [paper](http://arxiv.org/pdf/1612.08083v1)
* CNN beats LSTM in language model?

### Model
![Model](http://img.blog.csdn.net/20161230161832163?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* Word embedding
* Hidden layers: $h_l(X)=(X*W+b) \bigotimes \sigma(X*V+c)$
    * $\bigotimes$: element-wise product
    * $W,V \in \mathbb{R}^{k\times m\times n}$
    * Note that the begin of the sequence is zero-padded by `k/2`
    * The linear gate can alleviate vanishing gradient problem (can be seen as a multiplicative skip connection)
* Adaptive softmax

### Experiment
* Datasets: Google Billion Word (GBW), WikiText-103
* Optimization: Nesterov's momentum, gradient clipping (to 0.1), weight normalization
    * Can gain stable and fast convergence with large learning rate such as 1
* Hyper-parameter search
* Result
    * Speed: GCNN-22 compared with LSTM-2048 (units), better throughput and responsiveness
    * Complexity: GCNN-22 compared with LSTM-2048 (units), less parameter and FLOPs/token
    * Gating mechanism: GLU > (GTU (LSTM unit) <=> ReLU) > Tanh
    * Non-linear modeling: GLU > Linear > Bilinear
    * Network depth: the deep the better
    * Context: large context size brings lower test perplexity but returns diminish.


## Learning from Simulated and Unsupervised Images through Adversarial Training
* [paper](http://arxiv.org/pdf/1612.07828v1.pdf)
* This is (probably?) the first paper from Apple in ML/CV field

### Contribution
* Propose Simulate + Unsupervised training, using real world images to refine the synthetic images, with GAN
* Training GAN using adversarial loss and a self-regularization loss
* Key modifications to stabilize GAN training and prevent artifacts

### Framework
* Refiner (Generator): ![equation](http://latex.codecogs.com/svg.latex?%24%5Ctilde%7Bx%7D%3A%3DR_%5Ctheta%28x%29%24)
* Refiner loss (general formular): ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D_R%28%5Ctheta%29%3D%5Csum_i%20l_%7Breal%7D%28%5Ctheta%3B%20%5Ctilde%7Bx%7D_i%2C%20%5Cmathcal%7BY%7D%29%2B%5Clambda%20l_%7Breg%7D%28%5Ctheta%3B%20%5Ctilde%7Bx%7D_i%2C%20x_i%29)
    * ![equation](http://latex.codecogs.com/svg.latex?l_%7Breg%7D) minimizing the difference between the synthetic and the refined images
* Discriminator Loss: ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D_D%28%5Cphi%29%20%3D%20-%5Csum_i%20%5Clog%28D_%7B%5Cphi%7D%20%28%5Ctilde%7Bx%7D_i%29%29%20-%20%5Csum_j%20%5Clog%281-D_%7B%5Cphi%7D%28y_j%29%29)
    * `x~_i, y_j` are randomly sampled from refined images and real images sets
* Algorithm:
    * ![algorithm](http://img.blog.csdn.net/20161229114626218?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* `L_R` loss in the implementation of this paper: ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D_R%28%5Ctheta%29%3D%5Csum_i%20%5Clog%281-D_%7B%5Cphi%7D%28R_%7B%5Ctheta%7D%28x_i%29%29%29%2B%5Clambda%7C%7CR_%7B%5Ctheta%7D%28x_i%29-x_i%7C%7C_1)

### Stabilize GAN training
* Local adversarial loss: divide the refined image and real image into `w x h` regions and use separate discriminators to judge each region
    * ![local adversarial loss](http://img.blog.csdn.net/20161229115211830?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    * Final loss is the sum of loss on each region
* Using history of refined images
    * Two issues when only use the latest refined images
        * Diverging of adversarial training
        * The refiner network re-introducing the artifacts that the discriminator had forgotten about
    * Buffer history images, use `b/2` history images and `b/2` newly refined images in each iter
    * Update `b/2` of the buffered images in each iter

### Experiments
* Gaze estimation
    * Dataset: MPIIGaze dataset
    * Synthesizer: UnityEyes
    * Visual Turing test: human cannot tell the difference between refined and real images
    * Quantitative result: 22.3% percentage of improvement
* Hand pose estimation
    * Dataset: NYU hand pose dataset
    * Training CNN: Hour glass network


## Feature Pyramid Networks for Object Detection
* [paper](http://arxiv.org/pdf/1612.03144v1)

### Intuition
* Multi-scale is important in traditional methods
* Current detection system use single shot CNN to save time and memory (Faster R-CNN)
* CNN is capable of representing higher-level semantics, but not all levels are semantically strong
* Single Shot Detector (SSD) is one of the first attempts at using ConvNet's pyramidal feature, but they add new layers after high up layer, which may lose information in high-resolution feature map
* Main contribution: perform multi-scale in the network

### Model
* Feature Pyramid Network (FPN) building block
    * ![FPN block](http://img.blog.csdn.net/20161227164614908?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    * The feature maps in the picture above are from the last layer of each **stage** in ConvNet
    * Nearest neighbor upsampling
    * Denotes final set of feature maps as ![equation](http://latex.codecogs.com/svg.latex?%24%5C%7BP_2%2C%20P_3%2C%20P_4%2C%20P_5%5C%7D%24%2C%20corresponding%20to%20%24%5C%7BC_2%2C%20C_3%2C%20C_4%2C%20C_5%5C%7D%24)
* FPN in Region Proposal Network (RPN)
    * Replacing single-scale feature map with FPN
    * Anchors with different aspect ratios: 1:2, 1:1, 2:1
    * 15 anchors over the pyramid
* FPN in Fast R-CNN

### Experiment
* Evaluate on COCO `minival` set
* Surpass 2016 COCO winner
* Lateral and top-down connection is helpful

## RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation
* [paper](http://arxiv.org/pdf/1611.06612v3.pdf)
* [code](https://github.com/guosheng/refinenet)

### Intuition
* Conv and pooling result in the lost of finer image structure, producing low resolution segmentation.
    * DeepLab solve this by  atrous (or dilated) convolutions to account for larger receptive fields without downscaling the image.
    * FCN and Hypercolumns exploits features from intermediate layers for generating high-resolution prediction.
* Feature from all levels help to generate semantic segmentation.

### Model
* Compare with standard CNN and Dilated convolutions
    ![compare](http://img.blog.csdn.net/20161223103630473?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* Single RefineNet structure
    ![RefineNet](http://img.blog.csdn.net/20161223103656829?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* Purpose of different components in RefineNet
    * RCU: finetune feature map for fusion task
    * Multi-resolution fusion: scale to the same resolution and sum
    * Chained residual pooling: capture background context from a large image region
    * Output convolutions: another RCU. There are 3 RCU between two RefineNet blocks (2 path). The whole net also adds 2 RCU before dense softmax classifier.
* Residual connection: in RefineNet unit and **between RefineNet blocks**

### Experiment
* Datasets
    * **Person-Part** 1717 training and 1818 testing, human parsing
    * **NYUDv2** 795 training, 654 testing, RGB-D images showing interior scenes
    * **PASCAL VOC 2012** training/validation/test -- 1464/1449/1456 (usually trained with MS COCO dataset)
    * **Cityscapes** training/validation** 2975/500, 19 classes
    * **PASCAL-Context** segmentation labels of the whole scene for PASCAL VOC images
    * **SUN-RGBD** around 10,000 RGB-D indoor images, 37 classes
    * **ADE20K MIT** 150 classes, more than 20K scene images
* Measurement: IoU, pixel accuracy, mean accuracy
* Augmentation: random scaling, random cropping and horizontal flipping
* The result on PASCAL VOC 2012 is not better than [PSPNet](https://arxiv.org/pdf/1612.01105.pdf) (see the following paper note)
* Variant of model
    * ![variant](http://img.blog.csdn.net/20161223132208376?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    * 4-cascaded performs best but need more time


## Pyramid Scene Parsing Network
* [paper](https://arxiv.org/pdf/1612.01105.pdf)
* [project page](http://appsrv.cse.cuhk.edu.hk/~hszhao/projects/pspnet/index.html)
* [code](https://github.com/hszhao/PSPNet)
* 1-st place on ImageNet scene parsing challenge 2016

### Parsing overview
* Datasets
    * [LMO](http://people.csail.mit.edu/celiu/LabelTransfer/code.html)
    * [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)
    * [ADE20K dataset](http://groups.csail.mit.edu/vision/datasets/ADE20K/)
    * [Cityscapes](https://www.cityscapes-dataset.com/)
* FCN is the baseline model for deep learning based parsing
    * Research line 1: multi-scale feature ensembling
    * Research line 2: structure prediction
    * FCN has also been used in depth estimation, image restoration, image super-resolution.
* Some prior works use global image-level information for scene understanding

### Intuition
* FCN method suffers from mismatched relationship, confusion categories and inconspicuous classes
* Global average pooling fuses different stuff in a single vector and may lose the spatial relation. Global context information along with sub-region context may be more helpful.

### Model

![Model](http://appsrv.cse.cuhk.edu.hk/~hszhao/projects/pspnet/figures/pspnet.png)

* Pyramid pooling: bin size of 1x1, 2x2, 3x3, 6x6, both max and average pooling
* Auxiliary loss in ResNet-101

### Experiment
* Datasets: ImageNet scene parsing (ADE20K), PASCAL VOC 2012, Cityscapes
* Settings: poly learning rate, augmentation, momentum = 0.9 and weight decay = 0.0001.
* Large cropsize can get good performance (consistant with our experiment in ResNet)
* Batch size in batch normalization layer is important.
* Ablation study of settings
    * Average pooling is better than max
    * Pyramid is better than global pooling
    * Dimension reduction after pooling and concatenate is helpful
* Ablation study of auxiliary loss
    * `\alpha = 0.4` yields best performance
* Ablation study for ResNet depth
    * The deep the better
    * All ResNet is pre-trained on ImageNet
* Experiment on PASCAL VOC 2012
    * 10,582, 1,449 and 1,456 images for training, validation and testing
    * Top accuracy on all classes w/o MS COCO pre-training, top on most classes w/ MS COCO pre-training.
    * 85.4% mIoU.
* Cityscapes
    * 2,975, 500, and 1,525 for training, validation and testing, 19 categories containing both stuff and objects.
    * 20,000 coarsely annotated images, can be used for training.
    * Out-performs other methods with notable advantage (See project page for statistics)


## PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection
* [paper](http://cn.arxiv.org/pdf/1608.08021v3)
* Author: Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, and Minje Park
* Intel Imaging and Camera Technology

### Highlight
* Speed up (real-time) detection process with a more efficient feature extraction CNN, without lossing too much accuracy.
* This network is smaller and more efficient than ResNet and can be a substitution of the latter.

### Main structure
* C.ReLU: Concatenated ReLU in early stage of CNN to reduce the number of computation.
    * In early stages, output nodes tend to be paired, i.e. one node's activation is the opposite of another's.
    * C.ReLU reduce the output channels by half and concatenate with negation.
* Inception
    * Not yet been widely applied.
    * Cost-efficient building block for multi-scale representation.
    * 1x1 Conv can preserve the receptive field of the previous layer.
* Multi-scale representation like HyperNet
    * Concatenate the output of the last layer and two intermediate layers, whose size are 2x and 4x of the last layer.
    * Set 2x layer as reference scale, down-scaling (pooing) 4x layer, up-scaling (interpolation) the last layer.

### Experiment
* Training details
    * Add Batch normalization layers before all ReLU.
    * Plateau detection based learning rate policy.
        * Measure the moving average of loss
        * Decide as *on-plateau* if its improvement is below a threshold.
        * Decrease the learning rate by a constant factor when *on-plateau*.
    * Number of proposals = 200
* VOC2007 & VOC2012 performance
    ![performance](http://img.blog.csdn.net/20161216125702361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


## Aggregated Residual Transformations for Deep Neural Networks
* [paper](http://cn.arxiv.org/pdf/1611.05431v1)
* Author: [Saining Xie](http://vcl.ucsd.edu/~sxie/about/), [Ross Girshick](https://people.eecs.berkeley.edu/~rbg/), Piotr Dollar´, [Zhuowen Tu](http://pages.ucsd.edu/~ztu/), [Kaiming He](http://kaiminghe.com/)

### Model
* Solution space of Inception architecture is a strict subspace of the solution space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding.
* The transformations to be aggregated are all of the same topology.
* Equivalent forms:
    ![equivalent](http://img.blog.csdn.net/20161215130302222?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.

### Experiemnt
* See original paper for training details.
* Increasing cardinality is more efficient than increasing depth/width.
* ImageNet
    * 224x224: 20.4% top-1, 5.3% top-5
    * 320x320/crop-to-299x299: 19.1% top-1, 4.4 top-5
* ImageNet 5K (5000 class)
* CIFAR
    * -10 3.58%
    * -100 17.31%
* COCO detection
    * 51.9 AP@0.5
    * 30.0 AP


## Xception: Deep Learning with Depthwise Separable Convolutions
* [paper](http://cn.arxiv.org/pdf/1610.02357v2)

### Intuition
* Inception series
* Conv maps cross-channel correlation and spatial correlation at the same time.
* Inception module makes this process easier and more efficient by explicitly factoring it into a series of operations that would independently look at cross-channel correlations and at spatial correlations.
* 1x1 Conv -> cross-channel correlation; 3x3 & 5x5 Conv -> spatial correlation.
    ![Inception](http://img.blog.csdn.net/20161213145839016?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* An extreme version of this separation is to entirely decouple the cross-channel and spatial operations, naming **Xception**.

### Xception
* Xception module:
    ![Xception](http://img.blog.csdn.net/20161213145912704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* First use 1x1 Conv
* Conduct depthwise separable convolution (DSC): each feature-map have different 3x3 Conv, then concatenate the result of each Conv.
* Advantages: Efficient parameter usage
* Whole model
    ![Model](http://img.blog.csdn.net/20161213145936293?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### Experiment
* Dataset: JET (internal Google dataset), ImageNet, FastEval14k.
* Result
    * Xception converges faster than Inception V3 and gets higher accuracy.
    * 21.0% top-1, 5.5% top-5 error on ImageNet.
    * Better with residual connection.
    * Worse with non-linear in between the 1x1 and DSC.


## Human Attribute Recognition by Deep Hierarchical Contexts
* [ECCV 2016](http://www.eccv2016.org/main-conference/)
* [paper](http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf)
* [poster](http://www.eccv2016.org/files/posters/P-4A-21.pdf)
* Author: Yining Li, [Chen Huang](http://mmlab.ie.cuhk.edu.hk/html_people/research_fellow_Chen_Huang.html), [Chen Change Loy](http://personal.ie.cuhk.edu.hk/~ccloy/), and [Xiaoou Tang](https://www.ie.cuhk.edu.hk/people/xotang.shtml)
* [CUHK multimedia lab](http://mmlab.ie.cuhk.edu.hk/index.html)

### Model
* Use VGG-16.
* Input whole image and conduct ROI pooling to get features for different parts.
* Four different VGGs:
    * Whole image;
    * Person bbox;
    * Part-based bbox;
    * Similar bbox from other persons.
* Attribute cross entropy loss.
* Details:
    * Use ground truth person bbox
    * Use [Poselet](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/poselets/) to generate part bbox

### Datasets
* Propose [WIDER attribute](http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html).
* [Berkeley Person Attribute](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/poselets/).
* [HAT](https://jurie.users.greyc.fr/datasets/hat.html).

### Experiment
* Current state-of-the-art on BPA, HAT and WIDER.


## Densely Connected Convolutional Networks
* [Code](https://github.com/liuzhuang13/DenseNet)
* [paper](http://cn.arxiv.org/pdf/1608.06993v3)
* A dense block with 5 layers and growth rate 4:
![illustration](https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg)

### Intuition
* Current trend of CNN architecture: create short paths from early layers to later layers.
    * ResNet
    * Highway network: The first network with more than 100 layers, bypassing paths
    * Stochastic depth: Improves the training of deep residual networks by dropping layers randomly during training, which manages to train a 1202-layer ResNet
    * FractalNets
* Wide filter is helpful.
* Connect all layers with each other.
* Combine features by concatenating them (ResNet combines by summation).
* DenseNet layers are very narrow (12 feature-maps per layer), resulting in less parameters

### Model
![dense block](https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg)

* Dense connectivity: concatenate all the preceding layers:
    ![equation](http://latex.codecogs.com/svg.latex?x_l%20%3D%20H_l%28%5Bx_0%2C%20x_1%2C%20%5Cdots%2C%20x_%7Bl-1%7D%5D%29)
* Composite function:
    `H_l` is defined as `BN + ReLU + 3x3 Conv`
* Pooling and dense block
    * See figure above
    * **Transition layer** between dense blocks, consist of `BN + 1x1 Conv + 2x2 AveragePooing`
* Growth rate `k`:
    * The number of output feature-maps.
    * The `l`-th layer will have `k x (l-1) + k_0` input feature-maps (`k_0`:input image channels)
* Bottleneck layers
    * Introduce 1x1 Conv before 3x3 Conv to reduce number of feature-maps will improve computation efficiency.
    * `H_l` is changed to `BN + ReLU + 1x1 Conv + BN + ReLU + 3x3 Conv`
    * `1x1 Conv` reduce the input to `4k` feature-maps in the experiment.
* Compression
    * Reduce feature-maps number in transition layer by factor ![equation](http://latex.codecogs.com/svg.latex?%5Ctheta)

### Experiment
* Datasets
    * CIFAR-10/100, 32x32
        * Zero-padded with 4 pixels on each side
        * Randomly cropped to again produce 32×32 images
        * Half of the images are then horizontally mirrored
    * SVHN (Street View House Numbers), 32x32
    * ImageNet, 224x224, 1.2m for training, 50000 for validation, 1000 classes
* Settings: weight decay 10e-4, Nesterov momentum of 0.9 w\o dampening, learning rate 0.1 with decay scheme, dropout when no data augmentation
* Accuracy result:
    * 3.46% on CIFAR-10, L=190, k=40
    * 17.18% on CIFAR-100, L=190, k=40
    * 1.59% on SVHN, L=100, k=24
* Capacity: the performance continues improving when L, k increase, showing the DenseNet is less prone to overfitting (???)
* Parameter efficiency.

### Comment
* By [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en).
* The reason that DenseNet would work is very likely to be the Pyramidal design (i.e. increasing number of filters along the network flow). Dense connection may not be superior according to his previous experiment. Other papers has demonstrate the effect of Pyramidal design, like Deep Pyramidal Residual Networks, which brings about an obvious 1~2% decrease in error rate on CIFAR.