## PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection
* [paper](http://cn.arxiv.org/pdf/1608.08021v3)
* Author: Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, and Minje Park
* Intel Imaging and Camera Technology

### Highlight
* Speed up (real-time) detection process with a more efficient feature extraction CNN, without lossing too much accuracy.
* This network is smaller and more efficient than ResNet and can be a substitution of the latter.

### Main structure
* C.ReLU: Concatenated ReLU in early stage of CNN to reduce the number of computation.
    * In early stages, output nodes tend to be paired, i.e. one node's activation is the opposite of another's.
    * C.ReLU reduce the output channels by half and concatenate with negation.
* Inception
    * Not yet been widely applied.
    * Cost-efficient building block for multi-scale representation.
    * 1x1 Conv can preserve the receptive field of the previous layer.
* Multi-scale representation like HyperNet
    * Concatenate the output of the last layer and two intermediate layers, whose size are 2x and 4x of the last layer.
    * Set 2x layer as reference scale, down-scaling (pooing) 4x layer, up-scaling (interpolation) the last layer.

### Experiment
* Training details
    * Add Batch normalization layers before all ReLU.
    * Plateau detection based learning rate policy.
        * Measure the moving average of loss
        * Decide as *on-plateau* if its improvement is below a threshold.
        * Decrease the learning rate by a constant factor when *on-plateau*.
    * Number of proposals = 200
* VOC2007 & VOC2012 performance
    ![performance](http://img.blog.csdn.net/20161216125702361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


## Aggregated Residual Transformations for Deep Neural Networks
* [paper](http://cn.arxiv.org/pdf/1611.05431v1)
* Author: [Saining Xie](http://vcl.ucsd.edu/~sxie/about/), [Ross Girshick](https://people.eecs.berkeley.edu/~rbg/), Piotr Dollar´, [Zhuowen Tu](http://pages.ucsd.edu/~ztu/), [Kaiming He](http://kaiminghe.com/)

### Model
* Solution space of Inception architecture is a strict subspace of the solution space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding.
* The transformations to be aggregated are all of the same topology.
* Equivalent forms:
    ![equivalent](http://img.blog.csdn.net/20161215130302222?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.

### Experiemnt
* See original paper for training details.
* Increasing cardinality is more efficient than increasing depth/width.
* ImageNet
    * 224x224: 20.4% top-1, 5.3% top-5
    * 320x320/crop-to-299x299: 19.1% top-1, 4.4 top-5
* ImageNet 5K (5000 class)
* CIFAR
    * -10 3.58%
    * -100 17.31%
* COCO detection
    * 51.9 AP@0.5
    * 30.0 AP


## Xception: Deep Learning with Depthwise Separable Convolutions
* [paper](http://cn.arxiv.org/pdf/1610.02357v2)

### Intuition
* Inception series
* Conv maps cross-channel correlation and spatial correlation at the same time.
* Inception module makes this process easier and more efficient by explicitly factoring it into a series of operations that would independently look at cross-channel correlations and at spatial correlations.
* 1x1 Conv -> cross-channel correlation; 3x3 & 5x5 Conv -> spatial correlation.
    ![Inception](http://img.blog.csdn.net/20161213145839016?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* An extreme version of this separation is to entirely decouple the cross-channel and spatial operations, naming **Xception**.

### Xception
* Xception module:
    ![Xception](http://img.blog.csdn.net/20161213145912704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* First use 1x1 Conv
* Conduct depthwise separable convolution (DSC): each feature-map have different 3x3 Conv, then concatenate the result of each Conv.
* Advantages: Efficient parameter usage
* Whole model
    ![Model](http://img.blog.csdn.net/20161213145936293?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2huMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### Experiment
* Dataset: JET (internal Google dataset), ImageNet, FastEval14k.
* Result
    * Xception converges faster than Inception V3 and gets higher accuracy.
    * 21.0% top-1, 5.5% top-5 error on ImageNet.
    * Better with residual connection.
    * Worse with non-linear in between the 1x1 and DSC.


## Human Attribute Recognition by Deep Hierarchical Contexts
* [ECCV 2016](http://www.eccv2016.org/main-conference/)
* [paper](http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf)
* [poster](http://www.eccv2016.org/files/posters/P-4A-21.pdf)
* Author: Yining Li, [Chen Huang](http://mmlab.ie.cuhk.edu.hk/html_people/research_fellow_Chen_Huang.html), [Chen Change Loy](http://personal.ie.cuhk.edu.hk/~ccloy/), and [Xiaoou Tang](https://www.ie.cuhk.edu.hk/people/xotang.shtml)
* [CUHK multimedia lab](http://mmlab.ie.cuhk.edu.hk/index.html)

### Model
* Use VGG-16.
* Input whole image and conduct ROI pooling to get features for different parts.
* Four different VGGs:
    * Whole image;
    * Person bbox;
    * Part-based bbox;
    * Similar bbox from other persons.
* Attribute cross entropy loss.
* Details:
    * Use ground truth person bbox
    * Use [Poselet](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/poselets/) to generate part bbox

### Datasets
* Propose [WIDER attribute](http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html).
* [Berkeley Person Attribute](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/poselets/).
* [HAT](https://jurie.users.greyc.fr/datasets/hat.html).

### Experiment
* Current state-of-the-art on BPA, HAT and WIDER.


## Densely Connected Convolutional Networks
* [Code](https://github.com/liuzhuang13/DenseNet)
* [paper](http://cn.arxiv.org/pdf/1608.06993v3)
* A dense block with 5 layers and growth rate 4:
![illustration](https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg)

### Intuition
* Current trend of CNN architecture: create short paths from early layers to later layers.
    * ResNet
    * Highway network: The first network with more than 100 layers, bypassing paths
    * Stochastic depth: Improves the training of deep residual networks by dropping layers randomly during training, which manages to train a 1202-layer ResNet
    * FractalNets
* Wide filter is helpful.
* Connect all layers with each other.
* Combine features by concatenating them (ResNet combines by summation).
* DenseNet layers are very narrow (12 feature-maps per layer), resulting in less parameters

### Model
![dense block](https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg)

* Dense connectivity: concatenate all the preceding layers:
    ![equation](http://latex.codecogs.com/svg.latex?x_l%20%3D%20H_l%28%5Bx_0%2C%20x_1%2C%20%5Cdots%2C%20x_%7Bl-1%7D%5D%29)
* Composite function:
    `H_l` is defined as `BN + ReLU + 3x3 Conv`
* Pooling and dense block
    * See figure above
    * **Transition layer** between dense blocks, consist of `BN + 1x1 Conv + 2x2 AveragePooing`
* Growth rate `k`:
    * The number of output feature-maps.
    * The `l`-th layer will have `k x (l-1) + k_0` input feature-maps (`k_0`:input image channels)
* Bottleneck layers
    * Introduce 1x1 Conv before 3x3 Conv to reduce number of feature-maps will improve computation efficiency.
    * `H_l` is changed to `BN + ReLU + 1x1 Conv + BN + ReLU + 3x3 Conv`
    * `1x1 Conv` reduce the input to `4k` feature-maps in the experiment.
* Compression
    * Reduce feature-maps number in transition layer by factor ![equation](http://latex.codecogs.com/svg.latex?%5Ctheta)

### Experiment
* Datasets
    * CIFAR-10/100, 32x32
        * Zero-padded with 4 pixels on each side
        * Randomly cropped to again produce 32×32 images
        * Half of the images are then horizontally mirrored
    * SVHN (Street View House Numbers), 32x32
    * ImageNet, 224x224, 1.2m for training, 50000 for validation, 1000 classes
* Settings: weight decay 10e-4, Nesterov momentum of 0.9 w\o dampening, learning rate 0.1 with decay scheme, dropout when no data augmentation
* Accuracy result:
    * 3.46% on CIFAR-10, L=190, k=40
    * 17.18% on CIFAR-100, L=190, k=40
    * 1.59% on SVHN, L=100, k=24
* Capacity: the performance continues improving when L, k increase, showing the DenseNet is less prone to overfitting (???)
* Parameter efficiency.

### Comment
* By [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en).
* The reason that DenseNet would work is very likely to be the Pyramidal design (i.e. increasing number of filters along the network flow). Dense connection may not be superior according to his previous experiment. Other papers has demonstrate the effect of Pyramidal design, like Deep Pyramidal Residual Networks, which brings about an obvious 1~2% decrease in error rate on CIFAR.