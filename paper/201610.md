## Human Re-identification in Crowd Videos using Personal, Social and Environmental (PSE) Constraints
* [ECCV 2016](http://www.eccv2016.org/main-conference/)
* [paper](http://crcv.ucf.edu/papers/eccv2016/AssariIdreesShah_ECCV16_ReIdCrowds.pdf)
* [poster](http://www.eccv2016.org/files/posters/P-1B-12.pdf)


## Deep Attributes Driven Person Re-identification
* [ECCV 2016](http://www.eccv2016.org/main-conference/)
* [paper](https://arxiv.org/pdf/1605.03259v2.pdf)
* [poster](http://www.eccv2016.org/files/posters/P-1B-34.pdf)
* Author: Chi Su, [Shiliang Zhang](http://www.idm.pku.edu.cn/staff/zhangshiliang/team/team.html), Junliang Xing, Wen Gao1, and [Qi Tian](http://www.cs.utsa.edu/~qitian/)

### Intuition
* Low level feature sensitive to viewpoint, body poses, etc., and have different character corresponding to different metric learning methods.
* Attribute is mid-level feature.
* Problem:  it is difficult to acquire enough training data for a large set of attributes.
* Related work: deep attribute learning works.
### Three stage model
![three stage model](http://img.blog.csdn.net/20161028143951960)

* Goal: learn an attribute predict model for person ReID through dCNN training
	![equation](http://latex.codecogs.com/svg.latex?A_I%3D%5Cmathcal%7BO%7D%28I%29)
* First stage: train on attribute dataset
	* AlexNet
	* Dataset ![equation](http://latex.codecogs.com/svg.latex?T%3D%5C%7Bt_1%2Ct_2%2C%5Cdots%2Ct_N%5C%7D) labeled with a binary attribute.
	* Get trained model ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BO%7D%5E%7BS1%7D), which could predict attribute for any test sample but lack discriminative power.
* Second stage: fine-tuning on person ID dataset
	* Dataset ![equation](http://latex.codecogs.com/svg.latex?U%3D%5C%7Bu_1%2Cu_2%2C%5Cdots%2Cu_M%5C%7D) has person ID label.
	* Only set attributes with top ![equation](http://latex.codecogs.com/svg.latex?p%3D10) highest confidence score as 1, others as 0.
	* Triplet loss
		![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%20%3D%20%5Csum_e%5EE%5C%7B%5Cmax%280%2C%20D%28A%5E%7B%28e%29%7D_%7B%28a%29%7D%2C%20A%5E%7B%28e%29%7D_%7B%28p%29%7D%29%2B%5Ctheta-D%28A%5E%7B%28e%29%7D_%7B%28a%29%7D%2C%20A%5E%7B%28e%29%7D_%7B%28n%29%7D%29%29%2B%5Cgamma%5Ctimes%20%5Cvarepsilon%5C%7D)
		![equation](http://latex.codecogs.com/svg.latex?%5Cvarepsilon%3DD%28A%5E%7B%28e%29%7D_%7B%28a%29%7D%2C%20%5Ctilde%7BA%7D%5E%7B%28e%29%7D_%7B%28a%29%7D%29%2BD%28A%5E%7B%28e%29%7D_%7B%28p%29%7D%2C%20%5Ctilde%7BA%7D%5E%7B%28e%29%7D_%7B%28p%29%7D%29%2BD%28A%5E%7B%28e%29%7D_%7B%28n%29%7D%2C%20%5Ctilde%7BA%7D%5E%7B%28e%29%7D_%7B%28p%29%7D%29)
		where ![equation](http://latex.codecogs.com/svg.latex?%5Ctheta%3D1), ![equation](http://latex.codecogs.com/svg.latex?%5Cgamma%3D0.01) in the experiment setting.
	* Get model ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BO%7D%5E%7BS2%7D)
* Third stage: fine-tuning on the Combined Dataset
	* First predict attributes for dataset ![equation](http://latex.codecogs.com/svg.latex?U) using ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BO%7D%5E%7BS2%7D), act as ground truth attributes.
	* Combine ![equation](http://latex.codecogs.com/svg.latex?T) and ![equation](http://latex.codecogs.com/svg.latex?U) as a big attribute dataset, training to get final *deep attribute* extractor ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BO%7D)

### Experiment
* Attribute dataset in first stage: PETA, split multi-class attribute into binary attributes.
* Tracking dataset in second stage: MOT challenge
* Attribute accuracy
	![attribute accuracy](http://img.blog.csdn.net/20161028151836383)
* Evaluate: VIPeR, PRID, GRID, Market-1501 (VIPeR, GRID and PRID are included in the PETA dataset, they will be excluded from PETA during training)
* XQDA metric learning method, further improvement
* Datasets: VIPeR (43.5%, `sor`->63.9%), PRID 450S (22.6%, `sor`->60%+), Market (Single: 39.4%, multiple: 49.0%, `sor`->78%)
	* `sor` denotes state-of-the-art
	* The model does not use these three dataset for training, thus it cannot compare directly with supervised re-id model.
* Additional experiments
	* Combine hand-crafted feature with *deep attribute*, improve about 8% on VIPeR.
	* Directly fine-tune FC7 feature of AlexNet, *deep attribute* performs better.


## Human-In-The-Loop Person Re-Identification
* [ECCV 2016](http://www.eccv2016.org/main-conference/)
* [Paper](http://www.eecs.qmul.ac.uk/~xz303/papers/ECCV16/WangEtAl_ECCV2016.pdf)
* [Poster](http://www.eccv2016.org/files/posters/P-2B-41.pdf)
* Author: [Hanxiao Wang](http://www.eecs.qmul.ac.uk/~hw304/#bio), [Shaogang Gong](http://www.eecs.qmul.ac.uk/~sgg/), Xiatian Zhu, [Tao (Tony) Xiang](https://www.eecs.qmul.ac.uk/~txiang/)

> Professor Shaogang Gong from [Queen Mary, University of London](http://www.eecs.qmul.ac.uk/), who works closely with Dr. Tony Xiang, is an expert in person re-id field. He wrote a book naming [Person Re-Identification](http://link.springer.com/book/10.1007%2F978-1-4471-6296-4). Since this book has been published, supervised learning method with CNN feature extractor has gradually dominate this field (person re-id). However, prof. Gong and his group are seeking for novel ways to resolve re-id problem. They have two papers about person re-id in ECCV 2016, together with [Person Re-identification by Unsupervised L1 Graph Learning](http://www.eecs.qmul.ac.uk/~sgg/papers/KodirovEtAl_ECCV2016.pdf), both do not follow the supervised learning scheme.

![model pipe line](http://img.blog.csdn.net/20161028124735051)

### Highlight
* Propose Human Verification Incremental Learning (HVIL), an **online learning** approach for person re-id.
* Do not need labeled data. Human participates in the training procedure, to give a pair of probe-gallery image a feedback as **true, similar(but not true), dissimilar**.
* Small training set, large test set.
* They show some other works attempting to relax the need of labeling, with semi-supervised, unsupervised and transfer learning approches, in Related Work part.

### Modeling human feedback as a loss function
* Incrementally optimised ranking function
	![equation](http://latex.codecogs.com/svg.latex?err%28f_%7Bx%5Ep%7D%28x%5Eg%29%2C%20y%29%3D%5Cmathcal%7BL%7D_y%28rank%28f_%7Bx%5Ep%7D%28x%5Eg%29%29%29)
	where ![equation](http://latex.codecogs.com/svg.latex?f_%7Bx%5Ep%7D%28x%5Eg%29) is the distanse of pair ![equation](http://latex.codecogs.com/svg.latex?%5C%7Bx%5Ep%2C%20x_g%5C%7D), which is defined as negative [Mahalanobis Distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). ![equation](http://latex.codecogs.com/svg.latex?y) denotes the feedback, that is, ![equation](http://latex.codecogs.com/svg.latex?y%5Cin%20) {true-match, strong-negative, week-negative} ({m,s,w}). ![equation](http://latex.codecogs.com/svg.latex?rank) is just an int number denotes the rank of a gallery image.
* Re-id ranking loss ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D_y) is defined as
	![equation](http://latex.codecogs.com/svg.latex?%5Cbegin%7Balign%2A%7D%0A%09%5Cmathcal%7BL%7D_y%28k%29%26%3D%5Csum_%7Bi%3D1%7D%5Ek%20%5Calpha_i%09%5Cquad%20if%5Cquad%20y%5Cin%20%5C%7Bm%2Cw%5C%7D%5C%5C%0A%09or%20%26%3D%5Csum_%7Bi%3Dk%2B1%7D%5E%7Bn_g%7D%5Calpha_i%20%5Cquad%20if%20%5Cquad%20y%5Cin%20%5C%7Bs%5C%7D%0A%09%5Cend%7Balign%2A%7D)
	with ![equation](http://latex.codecogs.com/svg.latex?%5Calpha_1%5Cgeq%20%5Calpha_2%5Cgeq%20%5Cdots%20%5Cgeq%200)

### Real-time Model Update for Instant Feedback Reward
* Negative Mahalanobis Distance:
	![equation](http://latex.codecogs.com/svg.latex?f_%7Bx%5Ep%7D%28x%5Eg%29%3D-%5B%28x%5Ep-x%5Eg%29%5E%5Cmathbf%7BT%7DM%28x%5Ep-x%5Eg%29%5D%2C%20M%5Cin%20S%5Ed_%2B)
	![equation](http://latex.codecogs.com/svg.latex?S%5Ed_%2B) represents semi-definate matrix.
* Knowledge cumulation by online learning
	![equation](http://latex.codecogs.com/svg.latex?M_t%3D%5Cunderset%7BM%5Cin%20S%5Ed_%2B%7D%7B%5Carg%5Cmin%7D%5CDelta_F%28M%2CM_%7Bt-1%7D%29%2B%5Ceta%5Cmathcal%7BL%7D%5E%7B%28t%29%7D)
	This equation with ![equation](http://latex.codecogs.com/svg.latex?t) indicate that the matrix ![equation](http://latex.codecogs.com/svg.latex?M) in M-distance is learned in stages (knowledge cumulation). ![equation](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%5E%7B%28t%29%7D) is the loss of human feed back in ![equation](http://latex.codecogs.com/svg.latex?t) stage. ![equation](http://latex.codecogs.com/svg.latex?%5CDelta_F) is [Burg matrix divergence](https://en.wikipedia.org/wiki/Bregman_divergence)??
	![equation](http://latex.codecogs.com/svg.latex?%5CDelta_F%28M%2C%20M_%7Bt-1%7D%29%3Dtr%28MM_%7Bt-1%7D%5E%7B-1%7D%29-logdet%28MM_%7Bt-1%7D%5E%7B-1%7D%29)

### Metric ensemble learning
* When no human feedback is avilable.
* Idea: re-using pairs already verified by human
	![equation](http://latex.codecogs.com/svg.latex?f_%7Bij%7D%5E%7Bens%7D%3Df_%7Bx_i%5Ep%7D%5E%7Bens%7D%28x_j%5Eg%29%3D-d_%7Bij%7D%5E%7B%5Cmathbf%7BT%7D%7DWd_%7Bij%7D)
* Ideal ranking: ![equation](http://latex.codecogs.com/svg.latex?f_%7Bij%7D%5E%2A%3D0) for ![equation](http://latex.codecogs.com/svg.latex?c_i%3Dc_j) and ![equation](http://latex.codecogs.com/svg.latex?f_%7Bij%7D%5E%2A%3D-1) for ![equation](http://latex.codecogs.com/svg.latex?c_i%5Cneq%20c_j).

### Experiment
* Settings
	* For human feedback, 300 people/image probe; 1000 people/image gallery. Return top-50 in the rank list for feedback.
	* Max 3 rounds for each probe, result in 300-900 indicative verification.
* Claim that suer input will be 10-fold less.
* Better than other human-in-the-loop methods. Less feedback and search time.
* 56.1% on CUHK-03, 78% on Market-1501.
* Evaluate automated person re-id
	* 168 pairs on CUHK-03, 234 pairs on Market-1501; supervised model trained with 300 ground truth data for comparison.
	* Also compared with unensembled matrix after ![equation](http://latex.codecogs.com/svg.latex?%5Ctau) (![equation](http://latex.codecogs.com/svg.latex?M_%7B%5Ctau%7D)) and average matrix ![equation](http://latex.codecogs.com/svg.latex?M) for all time ![equation](http://latex.codecogs.com/svg.latex?1-%5Ctau) (![equation](http://latex.codecogs.com/svg.latex?M_%7Bavg%7D))
	* Ensembled performs best.


## Embedding Deep Metric for Person Re-identification: A Study Against Large Variation
* ECCV 2016
* Author: [Hailin Shi](http://www.cbsr.ia.ac.cn/users/hailinshi/), Yang Yang, Xiangyu Zhu, Shengcai Liao, Zhen Lei, Weishi Zheng, [Stan Z. Li](http://www.cbsr.ia.ac.cn/users/szli/)

### Overview
* Re-id research topics:
	* Improving discriminative features.
	* Good metric for comparison.
	* This paper mainly focus on learning good metrics.
* Influenced by face recognition method (the author also works on face recognition).
* Contributions:
	* **Moderate Positive Mining**, a novel positive sample selection strategy for training CNN while the data has large intra-class variations.
	* **Metric weight constraint** (combine Euclidean distance with Mahalanobis distance).

### Moderate positive mining
* Intuitions
	* Positive samples with large distance is harmful.
	* Positive samples with too little distance have little contribution to convergance.
	* What to do: reduce the intra-class variance while preserving the intrinsic graphical structure of pedestrian data via mining the moderate positive pairs in the local range (picture).
	![<picture here>](http://img.blog.csdn.net/20161025124254314)
* Algorithm of choosing moderate positive sample (picture)
	* Compute the distances of 1-all positive&negative samples
	* Mine the hardest negative sample (min distance negative), ![equation](http://latex.codecogs.com/svg.latex?distance%20%3D%20d%5E%2A)
	* Subset of positive samples where distance is larger than ![equation](http://latex.codecogs.com/svg.latex?d%5E%2A)
	* In this subset, find positive pair with min distance -- moderate positive
	![<picture here>](http://img.blog.csdn.net/20161025124317408)

### Metric weight constraint
* Euclidean distance shortcomings:
	* Sensitive to the scale?
	* Blind to the correlation across dimensions
	* Using the Mahalanobis distance is a better choice for multivariate metric, argued by other work
* Another FC after distance between features is calculated to gain Mahalanobis distance.
	* Get Mahalanobis distance
		* ![equation](http://latex.codecogs.com/svg.latex?d%28x_1%2C%20x_2%29%3D%5Csqrt%7B%28x_1-x_2%29%5E%5Cmathbf%7BT%7DM%28x_1-x_2%29%7D)
		* ![equation](http://latex.codecogs.com/svg.latex?M%3DWW%5E%5Cmathbf%7BT%7D) (ensure ![equation](http://latex.codecogs.com/svg.latex?M) is semi-definate matrix)
		* ![equation](http://latex.codecogs.com/svg.latex?d%28x_1%2C%20x_2%29%3D%7C%7CW%5E%5Cmathbf%7BT%7D%28x_1-x_2%29%7C%7C_2)
	* This can be implemented by an FC layer
		![equation](http://latex.codecogs.com/svg.latex?y%3Df%28W%5E%5Cmathbf%7BT%7Dx%29)
* Weight constraint
	* Euclidean better generalization ability, less discriminability.
	* Balance between Euclidean and Mahalanobis distance.
	* ![equation](http://latex.codecogs.com/svg.latex?M) should have large values at the diagonal (Euclidean) and small values elsewhere, by giving constraint:
		* ![equation](http://latex.codecogs.com/svg.latex?%7C%7CWW%5E%5Cmathbf%7BT%7D-I%7C%7C%5E2_F%5Cleq%20C)
	* Further combine the constraint into the loss function as a regularization term:
		* Triplet loss: ![equation](http://latex.codecogs.com/svg.latex?L%20%3D%20d%28x_1%2C%20x_2%5Ep%29%2B%5Bm-d%28x_1%2C%20x_2%5En%29%5D) (margin set to 2 in the experiment)
		* Regularization: ![equation](http://latex.codecogs.com/svg.latex?%5Chat%7BL%7D%3DL%2B%5Cfrac%5Clambda2%7C%7CWW%5E%5Cmathbf%7BT%7D-I%7C%7C%5E2_F) (tune ![equation](http://latex.codecogs.com/svg.latex?%5Clambda) to get the best trade-off)
		* Gradient w.r.t ![equation](http://latex.codecogs.com/svg.latex?W) is computed by ![equation](http://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20%5Chat%7BL%7D%7D%7B%5Cpartial%20W%7D%3D%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W%7D%2B%5Clambda%28WW%5E%5Cmathbf%7BT%7D-I%29W)

### CNN architecture
![<picture here>](http://img.blog.csdn.net/20161025124349065)

![CNN structure](http://img.blog.csdn.net/20161025124443754)

* 3 branches CNN
	* Original image 128x64 => 3x64x64 (with overlap)
	* Untied (unshared) filter between CNN branches to learn specific features from the different human body parts of pedestrian image.

### Experiments
* Their baseline is very weak (worse than CUHK-03 baseline)
* Three parts are analyzed
	* Moderate positive and hard negative (improve 10%+)
	* Weight constraint, tune on ![equation](http://latex.codecogs.com/svg.latex?%5Clambda) (![equation](http://latex.codecogs.com/svg.latex?%5Clambda) around ![equation](http://latex.codecogs.com/svg.latex?10%5E%7B-2%7D) gets good trade-off)
	* Tied or untied filters between branches (Untied a little better)
* Augmentation
	* Random translation
	* Randomly cropped (0-5 pixels) in horizon and vertical, and stretched to recover the size
* Datasets
	* CUHK03 (Rank-1: 61.32% with hand-crafted bbox, 52.09% with detected bbox)
	* CUHK01 + Market-1501 in training (Rank-1: 86.59%)
	* VIPeR (Rank-1: 43.39%)


## Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification

* Author: Rahul Rama Varior, NTU Singapore; Mrinal Haloi, Nanyang technological Universi; [Gang Wang](http://ihome.ust.hk/~gwang/)
* Picutre of the model

![2016_10_14_cebff7ce866e92a1c89e96dfc777d19](http://oa5omjl18.bkt.clouddn.com/2016_10_14_cebff7ce866e92a1c89e96dfc777d19.png "Add Description")

* Current state-of-the-art on [Market-1501](201609.md#market-1501).

### Contribution
1. Architecture of baseline siamese network for person re-id.
2. Matching gate between convolutional blocks.

### Matching gate (MG) structure
* Feature summarization
	* Aggregates the local feature along a horizontal stripe.
	* Deal with problem of changed view point (view point change in re-id typically in the horizontal direction, same parts are very likely to be along the same horizontal region).
	* Equation and dimention:
		![equation](http://latex.codecogs.com/svg.latex?y_%7Br1%7D%3Df%28w%2Ax_%7Br1%7D%29); ![equation](http://latex.codecogs.com/svg.latex?y_%7Br2%7D%3Df%28w%2Ax_%7Br2%7D%29)
		where ![equation](http://latex.codecogs.com/svg.latex?x_%7Bri%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%20c%5Ctimes%20h%7D) is the ![equation](http://latex.codecogs.com/svg.latex?r%5E%7Bth%7D) row of feature map. ![equation](http://latex.codecogs.com/svg.latex?w%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%20c%5Ctimes%20h%5Ctimes%20h%7D) (maybe ![equation](http://latex.codecogs.com/svg.latex?%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%20c%5Ctimes%201%5Ctimes%20h%7D)) and ![equation](http://latex.codecogs.com/svg.latex?y_%7Br1%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%201%5Ctimes%20h%7D) (maybe ![equation](http://latex.codecogs.com/svg.latex?%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%201%5Ctimes%20h%5Ctimes%20h%7D)).
* Feature similarity
	* Euclidean distance of ![equation](http://latex.codecogs.com/svg.latex?y_%7Br1%7D) and ![equation](http://latex.codecogs.com/svg.latex?y_%7Br2%7D)
		![equation](http://latex.codecogs.com/svg.latex?g_r%5Ei%3D%5Cexp%28%5Cfrac%7B-%28y_%7Br1%7D%5Ei-y_%7Br2%7D%5Ei%29%5E2%7D%7Bp_i%5E2%7D%29)
		where ![equation](http://latex.codecogs.com/svg.latex?g_r%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%201%5Ctimes%20h%7D)
	* ![equation](http://latex.codecogs.com/svg.latex?p_i) decides the variance of the Gaussian function, learnable, should set a higher initial value.
* Filtering and boosting the features
	* Repeat ![equation](http://latex.codecogs.com/svg.latex?g_r) c times horizontally to obtain ![equation](http://latex.codecogs.com/svg.latex?G_r%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%20c%5Ctimes%20h%7D).
	* Add filtered feature to original feature.
		![equation](http://latex.codecogs.com/svg.latex?a_%7Br1%7D%5Ei%3Dx_%7Br1%7D%5Ei%2Bx_%7Br1%7D%5Ei%5Codot%20G_r%5Ei)
		![equation](http://latex.codecogs.com/svg.latex?a_%7Br2%7D%5Ei%3Dx_%7Br2%7D%5Ei%2Bx_%7Br2%7D%5Ei%5Codot%20G_r%5Ei)
	* Perform L2 normalization across channels after this

### Result
* Dataset: Market-1501, CUHK-03, VIPeR.
* Baseline S-CNN outperform most CNN approaches. With MG gaining further improvement.
* Visualization of gate. Low gate activation means low similarity.
	![2016_10_14_abc0a52873dfd63db5fbe8ad154b87c](http://oa5omjl18.bkt.clouddn.com/2016_10_14_abc0a52873dfd63db5fbe8ad154b87c.png "Add Description")

## Joint Learning of Single-image and Cross-image Representations for Person Re-identification

![2016_10_14_57c771e82f299fbfd24bc07f946e486c](http://oa5omjl18.bkt.clouddn.com/2016_10_14_57c771e82f299fbfd24bc07f946e486c.png)

![2016_10_14_8d78afec4a3594b3eb45ba283cdf7](http://oa5omjl18.bkt.clouddn.com/2016_10_14_8d78afec4a3594b3eb45ba283cdf7.png)

### Intuition
* Re-id method: single-image representation (SIR) and cross-image representation (CIR).
* Combine them together!

### Method
* SIR measurements are special cases of CIR-based classification.
	* SIR(Euclidean distance): ![equation](http://latex.codecogs.com/svg.latex?S_%7BSIR%7D%28x_i%2Cx_j%29%3D%7C%7Cf%28x_i%29-f%28x_j%29%7C%7C_2%5E2)
	* CIR: ![equation](http://latex.codecogs.com/svg.latex?S_%7BCIR%7D%28x_i%2Cx_j%29%3Dw%5ETg%28x_i%2Cx_j%29-b)
* Also combine pairwise loss and triplet loss.
	* Pairwise
		* ![equation](http://latex.codecogs.com/svg.latex?L_%7BSIR%7D%5EP%3D%5Csum_%7Bi%2Cj%7D%5B1%2Bh_%7Bij%7D%28%7C%7Cf%28x_i%29-f%28x_j%29%7C%7C_2%5E2-b_%7BSIR%7D%29%5D_%2B)
		* ![equation](http://latex.codecogs.com/svg.latex?L_%7BCIR%7D%5EP%3D%5Cfrac%7B%5Calpha_P%7D%7B2%7D%7C%7Cw%7C%7C_2%5E2%2B%5Csum_%7Bi%2Cj%7D%5B1%2Bh_%7Bij%7D%28w%5ETg%28x_i%2Cx_j%29-b_%7BSIR%7D%29%5D_%2B)
		* where ![equation](http://latex.codecogs.com/svg.latex?b_%7BSIR%7D%2Cb_%7BCIR%7D) is the distance threshold (margin), ![equation](http://latex.codecogs.com/svg.latex?%5Calpha_P) is the trade-off parameter, which is set to 0.0005 in the experiments
		* Combine: ![equation](http://latex.codecogs.com/svg.latex?L%5EP%3DL_%7BSIR%7D%5EP%2B%5Ceta_PL_%7BCIR%7D%5EP)
	* Triplet
		* ![equation](http://latex.codecogs.com/svg.latex?L_%7BSIR%7D%5ET%3D%5Csum_%7Bi%2Cj%2Ck%7D%5Bb_%7BSIR%7D-%7C%7Cf%28x_i%29-f%28x_k%29%7C%7C_2%5E2%2B%7C%7Cf%28x_i%29-f%28x_j%29%7C%7C_2%5E2%29%5D_%2B)
		* ![equation](http://latex.codecogs.com/svg.latex?L_%7BCIR%7D%5ET%3D%5Cfrac%7B%5Calpha_P%7D%7B2%7D%7C%7Cw%7C%7C_2%5E2%2B%5Csum_%7Bi%2Cj%2Ck%7D%5Bb_%7BCIR%7D%2Bw%5ETg%28x_i%2Cx_k%29-w%5ETg%28x_i%2Cx_j%29%5D_%2B)
		Combine: ![equation](http://latex.codecogs.com/svg.latex?L%5ET%3DL_%7BSIR%7D%5ET%2B%5Ceta_TL_%7BCIR%7D%5ET)
* Compute cross-image feature map
	![equation](http://latex.codecogs.com/svg.latex?%5Cvarphi_r%28x_i%2Cx_j%29%3Dmax%280%2Cb_r%2B%5Csum_qk_%7Bq%2Cr%7D%2A%5Cphi_q%28x_i%29%2Bl_%7Bq%2Cr%7D%2A%5Cphi_q%28x_j%29%29)

### Result
* Dataset: CUHK-01/03, VIPeR.
* 52.17% on CUHK03.
* Investigation on sensitivity of trade-off parameter.
* 71.8% on CUHK01 (pretrain on CUHK03).
* 35.76% on VIPeR.

## Learning by tracking: Siamese CNN for robust target association

### Problem of tracking
* Most modern approaches use the tracking-by-detection paradigm:
	* Detecting pedestrians in the scene;
	* Link detections over time to create trajectories.
* Linking (data association) problem -- represent as a graph
	* Node: each detection
	* Edge: possible link
	* Data association can be formulated as maximum flow/minimum cost problem, solved with LP
	* Alternative formulations: minimum cliques, MCMC
* Recent trend of data association: complex model, using other technique like reconstruction for multicamera sequences, activity recognition and segmentation.