## Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
[paper link](https://arxiv.org/pdf/1506.01497v3.pdf)

## Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning
[paper link](http://arxiv.org/pdf/1511.03476v1.pdf)

![2016_09_01_f5f769c9ad886f8d209bf7d3a425969](http://oa5omjl18.bkt.clouddn.com/2016_09_01_f5f769c9ad886f8d209bf7d3a425969.png "Intuition")

![2016_09_01_90d89552e015c34e67806a4845e0b587](http://oa5omjl18.bkt.clouddn.com/2016_09_01_90d89552e015c34e67806a4845e0b587.png "Whole network")

> Re-read this to get the intuition of current working model.

### Intuitions

1. The author argue that the following solutions to extract temporal information are lack of long term temporal dependency. We need to model video temporal structure with multiple granularities.
	* Two-stream (optical flow is expensive and only short duration feature)
	* 3D Convnet ([Should have a link after reading this])
	* VLAD (know nothing)
2. LSTM can deal with long video, but the **favorable length is 30~80 frames**.
3. Additional non-linearity is helpful, so we can stack LSTMs.

### Model
Picture one, using stride (= 8) with attention as the input to the first layer lstm (number of steps set to 8)

## Microsoft Video Description Corpus (MSVD)
[dataset link](https://www.microsoft.com/en-us/download/details.aspx?id=52422)
[dataset paper link](http://www.cs.utexas.edu/~ml/papers/chen.acl11.pdf)

### Description
This data consists of about 120K sentences collected during the summer of 2010. Workers on Mechanical Turk were paid to watch a short video snippet and then summarize the action in a single sentence. The result is a set of roughly parallel descriptions of more than 2,000 video snippets. Because the workers were urged to complete the task in the language of their choice, both paraphrase and bilingual alternations are captured in the data. We expect this data to be useful for training and testing translation and paraphrase algorithms. A paper describing how the data was created and used is in progress.

### Scale
1970 clips (train/validate/test:1200/100/670), 80k clip-description pairs (multi-language)

### Duration
Usually less than 10 seconds.

### Examples

[![2016_09_01_227420e81de92e12b54e9aed867253](http://oa5omjl18.bkt.clouddn.com/2016_09_01_227420e81de92e12b54e9aed867253.png "Add Description")](https://youtu.be/mv89psg6zh4?t=33s)

* Ground truth:
	+ A bird in a sink keeps getting under the running water from a faucet.
	+ A bird is bathing in a sink.
	+ A bird is splashing around under a running faucet.
	+ A bird is bathing in a sink.
	+ A bird is standing in a sink drinking water that is pouring out of the facet.
	+ A faucet is running while a bird stands in the sink below.
	+ ......(many others)

* Some other video:
	+ [A man is riding a motorcycle.](https://youtu.be/msCidKHOh74?t=6m50s)
	+ [The man jumped on the wall.](https://youtu.be/ItFqogTmAvQ?t=48s)
	+ [A man is playing pat-a-cake with a small black dog.](https://youtu.be/eyhzdC936uk?t=15s)

## Youtube2Text
[dataset link](https://www.microsoft.com/en-us/download/details.aspx?id=52422)

This is an alias to [Montreal Video Annotation Dataset (M-VAD)](201609.md#montreal-video-annotation-dataset-m-vad) dataset.

## Montreal Video Annotation Dataset (M-VAD)
[dataset link](https://mila.umontreal.ca/en/publications/public-datasets/m-vad/)

## Tumblr GIF (TGIF)
[dataset link](https://github.com/raingo/TGIF-Release)
[dataset paper link](http://arxiv.org/pdf/1604.02748v2.pdf)
[Webpage](http://raingo.github.io/TGIF-Release/)

* Description: The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. We provide the URLs of animated GIFs in this release. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. We provide one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset shall be used to evaluate animated GIF/video description techniques.

