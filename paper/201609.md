## Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
* [paper link](https://arxiv.org/pdf/1506.01497v3.pdf)

## Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning
[paper link](http://arxiv.org/pdf/1511.03476v1.pdf)

![2016_09_01_f5f769c9ad886f8d209bf7d3a425969](http://oa5omjl18.bkt.clouddn.com/2016_09_01_f5f769c9ad886f8d209bf7d3a425969.png "Intuition")

![2016_09_01_90d89552e015c34e67806a4845e0b587](http://oa5omjl18.bkt.clouddn.com/2016_09_01_90d89552e015c34e67806a4845e0b587.png "Whole network")

> Re-read this to get the intuition of current working model.

### Intuitions

1. The author argue that the following solutions to extract temporal information are lack of long term temporal dependency. We need to model video temporal structure with multiple granularities.
	* Two-stream (optical flow is expensive and only short duration feature)
	* 3D Convnet ([Should have a link after reading this])
	* VLAD (know nothing)
2. LSTM can deal with long video, but the **favorable length is 30~80 frames**.
3. Additional non-linearity is helpful, so we can stack LSTMs.

### Model
* Picture one, using stride (= 8) with attention as the input to the first layer lstm (number of steps set to 8).
* Every next layer, use stride to divide previous output into several groups, and use attention on the group to make input.
* Whole model has 2 layers.

### Result
Compare with FGM, Meanpool, SA, S2VT, LSTM-E, p-RNN. The performance is similar with p-RNN on MSVD.

## Describing Videos by Exploiting Temporal Structure
* [paper link](https://arxiv.org/pdf/1502.08029v5.pdf)

The first paper using attention on video caption task.

![2016_08_11_76334843cd90cd7af03e2483df49e78](http://oa5omjl18.bkt.clouddn.com/2016_08_11_76334843cd90cd7af03e2483df49e78.png "Structure of video captioning system using temporal attention")

### Model
* [GoogLeNet](201608.md#going-deeper-with-convolutions) to extract frame feature.
* Extract temporal information in a video. Attention->global, 3D-CNN->local. Concatenate 3D-CNN and GoogLeNet feature together.
* 3D-CNN is pre-trained on activity recognition datasets.

### Result
* Do experiments w,w/o attention/3-D CNN, found that the improvements brought by exploiting global and local temporal information are complimentary.

## Microsoft Video Description Corpus (MSVD)
* [dataset link](https://www.microsoft.com/en-us/download/details.aspx?id=52422)
* [dataset paper link](http://www.cs.utexas.edu/~ml/papers/chen.acl11.pdf)

### Description
This data consists of about 120K sentences collected during the summer of 2010. Workers on Mechanical Turk were paid to watch a short video snippet and then summarize the action in a single sentence. The result is a set of roughly parallel descriptions of more than 2,000 video snippets. Because the workers were urged to complete the task in the language of their choice, both paraphrase and bilingual alternations are captured in the data. We expect this data to be useful for training and testing translation and paraphrase algorithms. A paper describing how the data was created and used is in progress.

### Scale
1970 clips (train/validate/test:1200/100/670), 80k clip-description pairs (**multi-language**), have **audio**.

### Duration
Usually less than 10 seconds.

### Examples

[![2016_09_01_227420e81de92e12b54e9aed867253](http://oa5omjl18.bkt.clouddn.com/2016_09_01_227420e81de92e12b54e9aed867253.png "Add Description")](https://youtu.be/mv89psg6zh4?t=33s)

* Ground truth:
	+ A bird in a sink keeps getting under the running water from a faucet.
	+ A bird is bathing in a sink.
	+ A bird is splashing around under a running faucet.
	+ A bird is bathing in a sink.
	+ A bird is standing in a sink drinking water that is pouring out of the facet.
	+ A faucet is running while a bird stands in the sink below.
	+ ......(many others)

* Some other video:
	+ [A man is riding a motorcycle.](https://youtu.be/msCidKHOh74?t=6m50s)
	+ [The man jumped on the wall.](https://youtu.be/ItFqogTmAvQ?t=48s)
	+ [A man is playing pat-a-cake with a small black dog.](https://youtu.be/eyhzdC936uk?t=15s)

### Papers using the dataset
* [Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning](201609.md#hierarchical-recurrent-neural-encoder-for-video-representation-with-application-to-captioning)
* [Describing Videos by Exploiting Temporal Structure](201609.md#describing-videos-by-exploiting-temporal-structure)

## Youtube2Text
* [dataset link](https://www.microsoft.com/en-us/download/details.aspx?id=52422)

This is an alias to [Microsoft Video Description Corpus (MSVD)](201609.md#microsoft-video-description-corpus-msvd) dataset.

## Montreal Video Annotation Dataset (M-VAD)
* [dataset link](https://mila.umontreal.ca/en/publications/public-datasets/m-vad/)

This dataset has a restricted access. Information not collected yet.

## Tumblr GIF (TGIF)
* [dataset link](https://github.com/raingo/TGIF-Release)
* [dataset paper link](http://arxiv.org/pdf/1604.02748v2.pdf)
* [web page](http://raingo.github.io/TGIF-Release/)

### Description
The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. We provide the URLs of animated GIFs in this release. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. We provide one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset shall be used to evaluate animated GIF/video description techniques.

### Scale
102,068 clips (train/validate/test: 80000/10780/11360). 1 caption/clip in train & validate; 3 in test.

### Duration
Overall 103 h, 3.65 s/clip.

### Examples

![gif](https://38.media.tumblr.com/a73a6a3c2667bb7a0bbbe9854c4f5d86/tumblr_ni2l89yZ7r1u4o6i5o1_500.gif)
a boy and a girl are walking down hand in hand.

![gif](https://33.media.tumblr.com/e24312fa8840e303c64c24a5e09e8381/tumblr_nghtio36IB1rp5w0lo1_400.gif)
a young woman is speaking to another lady and she looks off to the side.

![gif](https://33.media.tumblr.com/00b2f4b7137a0b272eecf86bbe4932c7/tumblr_nrgie93Udc1totisro1_250.gif)
a man strums a guitar with his mouth wide open.

### Papers using the dataset

## Microsoft Research - Video to Text (MSR-VTT)
* [dataset link](http://staff.ustc.edu.cn/~xinmei/Project/Project.html)
* [dataset link 2 (both can download)](http://ms-multimedia-challenge.com/dataset)
* [dataset paper link](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf)
[video to language challenge link](http://ms-multimedia-challenge.com/challenge)

![2016_09_02_f74a3b36446fc20141b4d9ac7a23d4d](http://oa5omjl18.bkt.clouddn.com/2016_09_02_f74a3b36446fc20141b4d9ac7a23d4d.png "Add Description")

### Description
MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers.

### Scale
Total 7180 (train/validate/test: 6513/497/170 ?), in train/validate each has 20 GT sentences. We haven't got the test set sentences yet.

### Duration
Total 41.2 h. The duration of each clip is between 10 and 30 seconds. Yet when I see those in validation set, they are less than 10 s?

### Examples
[see here](http://ms-multimedia-challenge.com/challenge)

## Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition

## Memory Networks
* [paper link](http://arxiv.org/abs/1410.3916v11)

### Structure
* **I(input feature map)**
	+ Standard pre-processing, e.g., parsing, co reference and entity resolution for text inputs
* **G(generalization)**
	+ Simplest form: ![equation](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bm%7D_%7BH%28x%29%7D%3DI%28x%29) store ![equation](http://latex.codecogs.com/svg.latex?I%28x%29) to a slot of memory.
	+ More sophisticated form: update all ![equation](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bm%7D) based on `x`
* **O(output feature map)**
	+ Find relevant memories
* **R(response)**
	+ Procedure the final response given `o`

### Procedure
1. Convert `x` to an internal feature representation `I(x)`
2. Update memories ![equation](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bm%7D_i) given the new input: ![equation](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bm%7D_i%20%3D%20G%28%5Cmathbf%7Bm%7D_i%2CI%28x%29%2C%5Cmathbf%7Bm%7D%29) for all `i`
3. Compute output features `o` given the new input and the memory: ![equation](http://latex.codecogs.com/svg.latex?o%20%3D%20O%28I%28x%29%2C%5Cmathbf%7Bm%7D%29)
4. Finally, decode output features `o` to give the final response: ![equation](http://latex.codecogs.com/svg.latex?r%20%3D%20R%28o%29)

### Use MemNN on Q&A task
#### Basic model
+ `I` = input text (sentence or word-based)
+ `G` = store new text into the next memory (![equation](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bm%7D_N%3Dx%2C%20N%3DN%2B1))
+ `O` = finding ![equation](http://latex.codecogs.com/svg.latex?k) relevant memories (they try ![equation](http://latex.codecogs.com/svg.latex?k%3D1%2C2))
![equation](http://latex.codecogs.com/svg.latex?o_1%3DO_1%28x%2C%5Cmathbf%7Bm%7D%29%3D%5Cmathop%7B%5Carg%5Cmax%7D%20%5Climits_%7Bi%3D1%2C2%2C...%2CN%7Ds_O%28x%2C%5Cmathbf%7Bm%7D_i%29)
![equation](http://latex.codecogs.com/svg.latex?o_2%3DO_2%28x%2C%5Cmathbf%7Bm%7D%29%3D%5Cmathop%7B%5Carg%5Cmax%7D%20%5Climits_%7Bi%3D1%2C2%2C...%2CN%7Ds_O%28%5Bx%2C%5Cmathbf%7Bm%7D_%7Bo_1%7D%5D%2C%5Cmathbf%7Bm%7D_i%29)
+ `R` = produce a textual response `r`
	* Return ![equation](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bm%7D_%7Bo_k%7D) (the last retrieved sentence)
	* Use RNN to generate text
	* Return single word by ranking them:
	![equation](http://latex.codecogs.com/svg.latex?r%3D%5Carg%5Cmax_%7B%5Comega%20%5Cin%20W%7Ds_R%28%5Bx%2C%5Cmathbf%7Bm%7D_%7Bo_1%7D%2C%5Cmathbf%7Bm%7D_%7Bo_2%7D%5D%2C%5Comega%29)
+ Score function in `O` and `R` (![equation](http://latex.codecogs.com/svg.latex?s_O%2C%20s_R))
![equation](http://latex.codecogs.com/svg.latex?s%28x%2C%20y%29%20%3D%20%20%5CPhi_x%28x%29%5E%7B%5Cmathbf%7BT%7D%7DU%5E%7B%5Cmathbf%7BT%7D%7DU%5CPhi_y%28y%29)
where ![equation](http://latex.codecogs.com/svg.latex?%5CPhi_x%2C%20%5CPhi_y) map original text to the D-dimensional feature space and ![equation](http://latex.codecogs.com/svg.latex?U) is the embedding matrix, ![equation](http://latex.codecogs.com/svg.latex?U%5Cin%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20D%7D)
+ Training
	* loss function
	![2016_09_15_868e7f2723d58af0b4287e94e345b8c7](http://oa5omjl18.bkt.clouddn.com/2016_09_15_868e7f2723d58af0b4287e94e345b8c7.png "Add Description")
	* Intuition: make ![equation](http://latex.codecogs.com/svg.latex?s_O%28x%2C%5Cmathbf%7Bm%7D_%7Bo_1%7D%29) larger than all ![equation](http://latex.codecogs.com/svg.latex?s_O%28x%2C%5Cbar%7Bf%7D%29) for at least ![equation](http://latex.codecogs.com/svg.latex?%5Cgamma).

## End-to-End Comparative Attention Networks for Person Re-identification
* [paper link](https://arxiv.org/pdf/1606.04404v1.pdf)

### Model structure
![2016_09_18_b2a7c855d76fd2d0fd9fc4ab8a7e92c](http://oa5omjl18.bkt.clouddn.com/2016_09_18_2016_09_18_b2a7c855d76fd2d0fd9fc4ab8a7e92c.png "2016_09_18_b2a7c855d76fd2d0fd9fc4ab8a7e92c")

![2016_09_18_b7ecb6331c92ce2ad9ae321dc346](http://oa5omjl18.bkt.clouddn.com/2016_09_18_b7ecb6331c92ce2ad9ae321dc346.png "Add Description")

* Use AlexNet Max5 feature (6 * 6 * 256)
* LSTM with attention
	* **Same image feature** at every time step (total time steps T=8)
	* Attention only conditioned on previous time step
	* Extract (2nd, 4th, 8th)/all/last hidden state and concatenated as the final features passed to the normalization layer (2,4,8 being the best)
* Re-id triplet method
	* Anchor I, positive I+, negative I-
	* Loss: ![2016_09_18_5740ab76c07ddde2b58ab5ef13af4081](http://oa5omjl18.bkt.clouddn.com/2016_09_18_5740ab76c07ddde2b58ab5ef13af4081.png "Re-id loss")

### Experiment
* Dataset: CUHK01, CUHK03, Market-1501
* Settings: see Sec.IV-B
* Result: Great improvements on all three datasets
ß