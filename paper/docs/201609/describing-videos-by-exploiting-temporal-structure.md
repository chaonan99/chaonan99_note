## Describing Videos by Exploiting Temporal Structure
* [paper link](https://arxiv.org/pdf/1502.08029v5.pdf)

The first paper using attention on video caption task.

![2016_08_11_76334843cd90cd7af03e2483df49e78](http://oa5omjl18.bkt.clouddn.com/2016_08_11_76334843cd90cd7af03e2483df49e78.png "Structure of video captioning system using temporal attention")

### Model
* [GoogLeNet](201608.md#going-deeper-with-convolutions) to extract frame feature.
* Extract temporal information in a video. Attention->global, 3D-CNN->local. Concatenate 3D-CNN and GoogLeNet feature together.
* 3D-CNN is pre-trained on activity recognition datasets.

### Result
* Do experiments w,w/o attention/3-D CNN, found that the improvements brought by exploiting global and local temporal information are complimentary.
