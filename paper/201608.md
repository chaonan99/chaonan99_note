## Visualizing and Understanding Convolutional Networks
[paper link](https://arxiv.org/pdf/1311.2901v3.pdf)

![2016_08_29_9fe8c0e8b9b8ac8f6dcfc189ae6cd1](http://oa5omjl18.bkt.clouddn.com/2016_08_29_9fe8c0e8b9b8ac8f6dcfc189ae6cd1.png "Model of this paper")

### Approach
Deconv net. Use a trained model, feed an image and deconv back to get feature map.
For different layers, method of deconv:

1. **Pooling** When forwarding the image, recording the locations of the maxima within each pooling region in a set of switch variables. Use switch for unpooling when backwarding.
2. **Relu** Relu again (*Why?*).
3. **Conv** Convolution operation with transposed version of the same filter (*Why?*).
4. **Contrast norm** Do not use any contrast normalization operations when in this
reconstruction path (this statement has been deleted in the newest version, *why?*)

### Result

1. The author improve the previous model and get better result on ImageNet, especially better features in 1st/2nd layers.
2. Extract feature map after different epoches, find that higher level features need more epoch to learn.
3. (Seems irrelevent to the visualization tool) Do geometric transformation and feed images to CNN, calculate Euclidean distance of features. Find that ConvNet has some invariance to translation (more invariance gained in higher layer) and scaling, but not on rotation, .

## Recurrent Models of Visual Attention
[paper link](https://arxiv.org/pdf/1406.6247v1.pdf)
> Tutorial and torch implementation [here](http://torch.ch/blog/2015/09/21/rmva.html).

![2016_08_29_18679d6f77d194f7a9030548b12133b](http://oa5omjl18.bkt.clouddn.com/2016_08_29_18679d6f77d194f7a9030548b12133b.png "Model of this paper")

### Highlights
1. They use [reinforcement learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/) on part of the network.
2. Their visual attention is to extract a series of bandlimited sub-images, concretely, use the same center and different sizes to get sub-images and scale them to the same size.

### Not finished
* Haven't seen the experiment result yet.

## Describing Multimedia Content using Attention-based Encoderâ€“Decoder Networks
[paper link](https://arxiv.org/pdf/1507.01053.pdf)

> A review on attention based encoder-decoder framework. Read this together with [this blog post](https://blog.heuritech.com/2016/01/20/attention-mechanism/).