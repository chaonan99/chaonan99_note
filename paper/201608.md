## Visualizing and Understanding Convolutional Networks
[paper link](https://arxiv.org/pdf/1311.2901v3.pdf)

![2016_08_29_9fe8c0e8b9b8ac8f6dcfc189ae6cd1](http://oa5omjl18.bkt.clouddn.com/2016_08_29_9fe8c0e8b9b8ac8f6dcfc189ae6cd1.png "Model of this paper")

### Approach
Deconv net. Use a trained model, feed an image and deconv back to get feature map.
For different layers, method of deconv:

1. **Pooling** When forwarding the image, recording the locations of the maxima within each pooling region in a set of switch variables. Use switch for unpooling when backwarding.
2. **Relu** Relu again (*Why?*).
3. **Conv** Convolution operation with transposed version of the same filter (*Why?*).
4. **Contrast norm** Do not use any contrast normalization operations when in this
reconstruction path (this statement has been deleted in the newest version, *why?*)

### Result

1. The author improve the previous model and get better result on ImageNet, especially better features in 1st/2nd layers.
2. Extract feature map after different epoches, find that higher level features need more epoch to learn.
3. (Seems irrelevent to the visualization tool) Do geometric transformation and feed images to CNN, calculate Euclidean distance of features. Find that ConvNet has some invariance to translation (more invariance gained in higher layer) and scaling, but not on rotation, .

## Recurrent Models of Visual Attention
[paper link](https://arxiv.org/pdf/1406.6247v1.pdf)
> Tutorial and torch implementation [here](http://torch.ch/blog/2015/09/21/rmva.html).

![2016_08_29_18679d6f77d194f7a9030548b12133b](http://oa5omjl18.bkt.clouddn.com/2016_08_29_18679d6f77d194f7a9030548b12133b.png "Model of this paper")

### Highlights
1. They use [reinforcement learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/) on part of the network.
2. Their visual attention is to extract a series of bandlimited sub-images, concretely, use the same center and different sizes to get sub-images and scale them to the same size.

### Reinforce training method
In their model, they generate location for glimpse in each time step. They are assuming that the location is sampled from a normal distribution parameterized by ![img](http://bit.ly/2bWdmLz) and ![img](http://bit.ly/2c6gnqs). They make ![img](http://bit.ly/2c6gnqs) to be fixed (thus hold a fixed attention band width), and try to learn propriate ![img](http://bit.ly/2bWdmLz), which is the average glimpse location (detailed formulations are shown in the blog post above). Only the last time step and the predicted class is correct will give R=1. In other cases, R=0.

### Not finished
* Haven't seen the experiment result yet.

## Describing Multimedia Content using Attention-based Encoderâ€“Decoder Networks
[paper link](https://arxiv.org/pdf/1507.01053.pdf)

> A review on attention based encoder-decoder framework. Read this together with [this blog post](https://blog.heuritech.com/2016/01/20/attention-mechanism/). Also [an NIPS workshop](http://www.thespermwhale.com/jaseweston/ram/) on this topic.

### Not finished
* Stop at Sec III-B. Read through when writing report for summer intern later.

## Going deeper with convolutions
[paper link](http://arxiv.org/pdf/1409.4842.pdf)

> Famous paper of **GoogLeNet**. Partly inspired by [Network In Network](201608.md#network-in-network). Related blog post [here](http://wikicoursenote.com/wiki/GoingDeeperWithConvolutions).

### Motivation

1. Improving the performance by increasing the **depth** (more layers) and **size** (more filters per layer). But this has two drawbacks:
	* More prone to overfitting.
	* Dramatically increase computation.
2. To solve this, consider replacing fully connected with **sparse** ones (Based on [Provable bounds for learning some deep representations](http://jmlr.org/proceedings/papers/v32/arora14.pdf)).
	* This is still problematic because today's computing infrastructures are inefficient on non-uniform sparse data structures.
3. Should use an architecture of **filter-level sparsity**.

### Inception module
![picture here](http://wikicoursenote.com/w/images/8/8f/I1.png "Inception module, naiive version")

![picture here](http://wikicoursenote.com/w/images/b/b1/I2.png "Inception module with dimensionality reduction")

1. Make the feature to be processed at various scales and then aggregated. (Easy to understand)
2. Use 1x1 convolutions to reduce dimensionality thus require less computation. (The paper said dense and compressded representation is hard to process. *Does this means reducing dimensionality will also improve the performance?*)
3. Guiding principles??? Still not clear why the structure works as suggested in the paper.

### Network architecture
See the picture in the paper.

1. Move from fully connected to average pooling.
2. Use dropout!!!
3. Add auxiliary classifiers in the middle
	* Make the lower layer also discriminative.
	* Provide regularization. (*Why?*)
	* These loss are added to the total loss with a discount weight.

### Training details on ILSVRC classification

1. 7 independent versions (only differ in sampling methodologies and the random order).
2. Cropping approach in **test stage**:
	* 4 scales of origin image, with shorter dimension sized 256, 288, 320, 352 respectively;
	* Each takes the top, center, bottom squares;
	* Each square takes 4 corners and the center 224x224 crop as well as the square resized to 224x224;
	* Each crop takes origin and mirror version.
	* Leads to 4x3x6x2 = 144 crops per image.
3. Softmax averaged over multiple crops and all the individual classifiers (7 models, not different classifiers in the same network) in **test stage**.

### Result
Winner of 2015 ILSVRC. 6.67% error on test set.

## Network in Network
[paper link](http://arxiv.org/pdf/1312.4400v3.pdf)